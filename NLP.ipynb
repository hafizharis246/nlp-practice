{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52988c8a-f8e0-4ca2-87a5-c0cbd5a7c02e",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49dbcfe8-35e2-45ac-a3e5-c9b8ccc04f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello! This is Hafiz Haris Mehmood, from Pakistan. \n",
    "Welcome to Data Services.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85658daf-ef88-4314-98b9-dd691329502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f202e7e8-98dd-487d-a8c1-a3eaf13de8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b39013d6-8acf-4071-9c5a-d506cc105644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'This is Hafiz Haris Mehmood, from Pakistan.', 'Welcome to Data Services.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b405349d-3709-43be-8850-7cf88a292c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "This is Hafiz Haris Mehmood, from Pakistan.\n",
      "Welcome to Data Services.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sent_tokenize(corpus):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "806f8642-4fc8-44d4-a112-8a81acb0c92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'This', 'is', 'Hafiz', 'Haris', 'Mehmood', ',', 'from', 'Pakistan', '.', 'Welcome', 'to', 'Data', 'Services', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d075f03c-c1f9-4323-bc8c-1c0e46a1a5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "!\n",
      "This\n",
      "is\n",
      "Hafiz\n",
      "Haris\n",
      "Mehmood\n",
      ",\n",
      "from\n",
      "Pakistan\n",
      ".\n",
      "Welcome\n",
      "to\n",
      "Data\n",
      "Services\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for words in word_tokenize(corpus):\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc5c8251-f54a-4f7c-b639-a1cd7c4de2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'Hafiz',\n",
       " 'Haris',\n",
       " 'Mehmood',\n",
       " ',',\n",
       " 'from',\n",
       " 'Pakistan',\n",
       " '.',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Data',\n",
       " 'Services',\n",
       " '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e5532-6a14-4b9f-afca-d4cf9bfd4ed1",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "## Porter Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "939c5d81-b1e8-42e8-97ba-1cc6af7aeb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b97af962-1d71-443f-b2db-4ed65c296fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\",\"fairly\", \"sportingly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3578ee5f-11ea-4273-acd0-8003aa4930d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->histori\n",
      "finally---->final\n",
      "finalized---->final\n",
      "fairly---->fairli\n",
      "sportingly---->sportingli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"---->\" + stemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d32aa1-6899-4883-b9e7-36382650960b",
   "metadata": {},
   "source": [
    "## Regexp Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "575768e1-7bf7-48ba-abf1-eb953562d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|ables$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7c28775c-20ae-431d-8690-68324546896d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "977d5301-b31a-4548-971f-e93e450af6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classmate'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"classmates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9cc63-a4cd-44d9-8d12-afb185d6d41d",
   "metadata": {},
   "source": [
    "## Snowball Stemmer\n",
    "gives better accuracy than porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3a8b47ad-0066-4460-86c5-497a97982a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ecff6a58-5d2e-41d8-a54e-b37e2f9a7626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->histori\n",
      "finally---->final\n",
      "finalized---->final\n",
      "fairly---->fair\n",
      "sportingly---->sport\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"---->\" + snowballstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df70d85-e0cb-4063-8d87-06024219b82c",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "## Wordnet Lemmatizer\n",
    "used for Q&A, Text Summarizations, Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "af468921-67a7-4454-a993-b6140d385da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1f4759e7-71c5-4aad-a8fe-48a85490f9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eating\n",
      "eats--->eats\n",
      "eaten--->eaten\n",
      "writing--->writing\n",
      "writes--->writes\n",
      "programming--->programming\n",
      "programs--->program\n",
      "history--->history\n",
      "finally--->finally\n",
      "finalized--->finalized\n",
      "fairly--->fairly\n",
      "sportingly--->sportingly\n"
     ]
    }
   ],
   "source": [
    "## for noun (by default noun)\n",
    "for word in words:\n",
    "    print(word + \"--->\" + lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "17e0ddc5-f99c-41ab-90bb-0e0c94b04ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eats--->eat\n",
      "eaten--->eat\n",
      "writing--->write\n",
      "writes--->write\n",
      "programming--->program\n",
      "programs--->program\n",
      "history--->history\n",
      "finally--->finally\n",
      "finalized--->finalize\n",
      "fairly--->fairly\n",
      "sportingly--->sportingly\n"
     ]
    }
   ],
   "source": [
    "## for verb\n",
    "for word in words:\n",
    "    print(word + \"--->\" + lemmatizer.lemmatize(word, \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "00b2690c-8e2a-4d7c-bd37-48c683f0fcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eating\n",
      "eats--->eats\n",
      "eaten--->eaten\n",
      "writing--->writing\n",
      "writes--->writes\n",
      "programming--->programming\n",
      "programs--->programs\n",
      "history--->history\n",
      "finally--->finally\n",
      "finalized--->finalized\n",
      "fairly--->fairly\n",
      "sportingly--->sportingly\n",
      "\n",
      "For Adjectives\n",
      "eating--->eating\n",
      "eats--->eats\n",
      "eaten--->eaten\n",
      "writing--->writing\n",
      "writes--->writes\n",
      "programming--->programming\n",
      "programs--->programs\n",
      "history--->history\n",
      "finally--->finally\n",
      "finalized--->finalized\n",
      "fairly--->fairly\n",
      "sportingly--->sportingly\n"
     ]
    }
   ],
   "source": [
    "## for adverb\n",
    "for word in words:\n",
    "    print(word + \"--->\" + lemmatizer.lemmatize(word, \"r\"))\n",
    "print(\"\\nFor Adjectives\")\n",
    "for word in words:\n",
    "    print(word + \"--->\" + lemmatizer.lemmatize(word, \"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4a5fd-3d75-49b3-93ca-e7e25f1ae931",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ed09486b-3a5f-4baa-8bc8-46d2c62ff5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "porterstemmer = PorterStemmer()\n",
    "snowballstemmer = SnowballStemmer(\"english\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5fb30e77-6383-4b4c-a74f-3d0fbeb0c34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## There are lot of stopwords in there, its better to create your own stopwords according to your need and then use it\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dc312ad1-fc8c-4c73-a632-bf547dc19c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds. From Alexander onwards the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours. Yet we have not done this to any other nation. We have not conquered anyone. We have not grabbed their land, their culture and their history and tried to enforce our way of life on them. Why? Because we respect the freedom of others. That is why my FIRST VISION is that of FREEDOM. I believe that India got its first vision of this in 1857, when we started the war of Independence. It is this freedom that we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "\n",
    "We have 10 percent growth rate in most areas. Our poverty levels are falling. Our achievements are being globally recognised today. Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect? MY SECOND VISION for India is DEVELOPMENT. For fifty years we have been a developing nation. It is time we see ourselves as a developed nation. We are among top five nations in the world in terms of GDP.\n",
    "\n",
    "I have a THIRD VISION. India must stand up to the world. Because I believe that unless India stands up to the world, no one will respect us. Only strength respects strength. We must be strong not only as a military power but also as an economic power. Both must go hand-in-hand. My good fortune was to have worked with three great minds. Dr.Vikram Sarabhai, of the Dept. of Space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material. I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
    "\n",
    "I was in Hyderabad giving this lecture, when a 14 year-old girl asked me for my autograph. I asked her what her goal in life is. She replied: I want to live in a developed India. For her, you and I will have to build this developed India. You must proclaim India is not an underdeveloped nation; it is a highly developed nation.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "964ff2da-496c-4952-9259-e9129d27ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenses = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cdedc7fc-6ec7-40b5-9b48-24a7b3f16f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "72be81f4-0555-4d57-ace9-c4c35d52cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords and Filter and then Apply Stemming\n",
    "for i in range(len(sentenses)):\n",
    "    words = nltk.word_tokenize(sentenses[i])\n",
    "    words = [porterstemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentenses[i] = ' '.join(words) #Converting words into sentences again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6c5e7516-4bc2-49ba-bd82-067231d64b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom other .',\n",
       " 'that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'my second vision india develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top five nation world term gdp .',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr.vikram sarabhai , dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i hyderabad give lectur , 14 year-old girl ask autograph .',\n",
       " 'i ask goal life .',\n",
       " 'she repli : i want live develop india .',\n",
       " 'for , i build develop india .',\n",
       " 'you must proclaim india underdevelop nation ; highli develop nation .']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7c916436-c64d-450b-90fa-22b7ceaa5866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three vision india .',\n",
       " '3000 year histori peopl world come invad us , captur land , conquer mind .',\n",
       " 'alexand onward greek , turk , mogul , portugu , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'conquer anyon .',\n",
       " 'grab land , cultur histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom .',\n",
       " 'first vision freedom .',\n",
       " 'believ india got first vision 1857 , start war independ .',\n",
       " 'freedom must protect nurtur build .',\n",
       " 'free , one respect us .',\n",
       " '10 percent growth rate area .',\n",
       " 'poverti level fall .',\n",
       " 'achiev global recogni today .',\n",
       " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
       " '’ incorrect ?',\n",
       " 'second vision india develop .',\n",
       " 'fifti year develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top five nation world term gdp .',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus believ unless india stand world , one respect us .',\n",
       " 'strength respect strength .',\n",
       " 'must strong militari power also econom power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortun work three great mind .',\n",
       " 'dr.vikram sarabhai , dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'lucki work three close consid great opportun life .',\n",
       " 'hyderabad give lectur , 14 year-old girl ask autograph .',\n",
       " 'ask goal life .',\n",
       " 'repli : want live develop india .',\n",
       " ', build develop india .',\n",
       " 'must proclaim india underdevelop nation ; high develop nation .']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply Stopwords and Filter and then Apply Snowball Stemming\n",
    "for i in range(len(sentenses)):\n",
    "    words = nltk.word_tokenize(sentenses[i])\n",
    "    words = [snowballstemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentenses[i] = ' '.join(words) #Converting words into sentences again\n",
    "sentenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "010bcdcc-4dce-4cb3-8ee5-41b7720afc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three visions india .',\n",
       " '3000 years history people world come invade us , capture land , conquer mind .',\n",
       " 'alexander onwards greeks , turks , moguls , portuguese , british , french , dutch , come loot us , take .',\n",
       " 'yet nation .',\n",
       " 'conquer anyone .',\n",
       " 'grab land , culture history try enforce way life .',\n",
       " '?',\n",
       " 'respect freedom others .',\n",
       " 'first vision freedom .',\n",
       " 'believe india get first vision 1857 , start war independence .',\n",
       " 'freedom must protect nurture build .',\n",
       " 'free , one respect us .',\n",
       " '10 percent growth rate areas .',\n",
       " 'poverty level fall .',\n",
       " 'achievements globally recognise today .',\n",
       " 'yet lack self-confidence see develop nation , self-reliant self-assured .',\n",
       " '’ incorrect ?',\n",
       " 'second vision india development .',\n",
       " 'fifty years develop nation .',\n",
       " 'time see develop nation .',\n",
       " 'among top five nations world term gdp .',\n",
       " 'third vision .',\n",
       " 'india must stand world .',\n",
       " 'believe unless india stand world , one respect us .',\n",
       " 'strength respect strength .',\n",
       " 'must strong military power also economic power .',\n",
       " 'must go hand-in-hand .',\n",
       " 'good fortune work three great mind .',\n",
       " 'dr.vikram sarabhai , dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear material .',\n",
       " 'lucky work three closely consider great opportunity life .',\n",
       " 'hyderabad give lecture , 14 year-old girl ask autograph .',\n",
       " 'ask goal life .',\n",
       " 'reply : want live develop india .',\n",
       " ', build develop india .',\n",
       " 'must proclaim india underdevelop nation ; highly develop nation .']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply Stopwords and Filter and then Apply Lemmatizing and making it into small aplhabets\n",
    "for i in range(len(sentenses)):\n",
    "    words = nltk.word_tokenize(sentenses[i])\n",
    "    words = [lemmatizer.lemmatize(word.lower(), pos = \"v\") for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentenses[i] = ' '.join(words) #Converting words into sentences again\n",
    "sentenses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa5cc4-4429-4af9-a2df-9e0da2dfca89",
   "metadata": {},
   "source": [
    "# Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2fbf0c47-9bf5-4d62-b826-a8345ad7fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history people from all over the world have come and invaded us, captured our lands, conquered our minds. From Alexander onwards the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours. Yet we have not done this to any other nation. We have not conquered anyone. We have not grabbed their land, their culture and their history and tried to enforce our way of life on them. Why? Because we respect the freedom of others. That is why my FIRST VISION is that of FREEDOM. I believe that India got its first vision of this in 1857, when we started the war of Independence. It is this freedom that we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "\n",
    "We have 10 percent growth rate in most areas. Our poverty levels are falling. Our achievements are being globally recognised today. Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect? MY SECOND VISION for India is DEVELOPMENT. For fifty years we have been a developing nation. It is time we see ourselves as a developed nation. We are among top five nations in the world in terms of GDP.\n",
    "\n",
    "I have a THIRD VISION. India must stand up to the world. Because I believe that unless India stands up to the world, no one will respect us. Only strength respects strength. We must be strong not only as a military power but also as an economic power. Both must go hand-in-hand. My good fortune was to have worked with three great minds. Dr.Vikram Sarabhai, of the Dept. of Space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material. I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
    "\n",
    "I was in Hyderabad giving this lecture, when a 14 year-old girl asked me for my autograph. I asked her what her goal in life is. She replied: I want to live in a developed India. For her, you and I will have to build this developed India. You must proclaim India is not an underdeveloped nation; it is a highly developed nation.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "96385b8a-e235-4c94-a8ee-dde30a8b4cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "496e8c3c-54f3-4875-bda9-370f51013c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('three', 'CD'), ('visions', 'NNS'), ('india', 'VBP'), ('.', '.')]\n",
      "[('3000', 'CD'), ('years', 'NNS'), ('history', 'NN'), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invade', 'VBP'), ('us', 'PRP'), (',', ','), ('capture', 'NN'), ('land', 'NN'), (',', ','), ('conquer', 'NN'), ('mind', 'NN'), ('.', '.')]\n",
      "[('alexander', 'NN'), ('onwards', 'NNS'), ('greeks', 'NN'), (',', ','), ('turks', 'NNS'), (',', ','), ('moguls', 'NNS'), (',', ','), ('portuguese', 'JJ'), (',', ','), ('british', 'JJ'), (',', ','), ('french', 'JJ'), (',', ','), ('dutch', 'VB'), (',', ','), ('come', 'VB'), ('loot', 'NN'), ('us', 'PRP'), (',', ','), ('take', 'VB'), ('.', '.')]\n",
      "[('yet', 'RB'), ('nation', 'NN'), ('.', '.')]\n",
      "[('conquer', 'NN'), ('anyone', 'NN'), ('.', '.')]\n",
      "[('grab', 'NN'), ('land', 'NN'), (',', ','), ('culture', 'NN'), ('history', 'NN'), ('try', 'NN'), ('enforce', 'NN'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('?', '.')]\n",
      "[('respect', 'NN'), ('freedom', 'NN'), ('others', 'NNS'), ('.', '.')]\n",
      "[('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
      "[('believe', 'VB'), ('india', 'NN'), ('get', 'NN'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('start', 'VBP'), ('war', 'NN'), ('independence', 'NN'), ('.', '.')]\n",
      "[('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
      "[('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
      "[('10', 'CD'), ('percent', 'JJ'), ('growth', 'NN'), ('rate', 'NN'), ('areas', 'NNS'), ('.', '.')]\n",
      "[('poverty', 'NN'), ('level', 'NN'), ('fall', 'NN'), ('.', '.')]\n",
      "[('achievements', 'NNS'), ('globally', 'RB'), ('recognise', 'VBP'), ('today', 'NN'), ('.', '.')]\n",
      "[('yet', 'RB'), ('lack', 'JJ'), ('self-confidence', 'NN'), ('see', 'NN'), ('develop', 'VB'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('self-assured', 'JJ'), ('.', '.')]\n",
      "[('’', 'NN'), ('incorrect', 'NN'), ('?', '.')]\n",
      "[('second', 'JJ'), ('vision', 'NN'), ('india', 'NN'), ('development', 'NN'), ('.', '.')]\n",
      "[('fifty', 'JJ'), ('years', 'NNS'), ('develop', 'VB'), ('nation', 'NN'), ('.', '.')]\n",
      "[('time', 'NN'), ('see', 'VB'), ('develop', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
      "[('among', 'IN'), ('top', 'JJ'), ('five', 'CD'), ('nations', 'NNS'), ('world', 'NN'), ('term', 'NN'), ('gdp', 'NN'), ('.', '.')]\n",
      "[('third', 'JJ'), ('vision', 'NN'), ('.', '.')]\n",
      "[('india', 'NN'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
      "[('believe', 'VB'), ('unless', 'IN'), ('india', 'JJ'), ('stand', 'NN'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
      "[('strength', 'NN'), ('respect', 'NN'), ('strength', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('strong', 'JJ'), ('military', 'JJ'), ('power', 'NN'), ('also', 'RB'), ('economic', 'JJ'), ('power', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('go', 'VB'), ('hand-in-hand', 'NN'), ('.', '.')]\n",
      "[('good', 'JJ'), ('fortune', 'NN'), ('work', 'NN'), ('three', 'CD'), ('great', 'JJ'), ('mind', 'NN'), ('.', '.')]\n",
      "[('dr.vikram', 'NN'), ('sarabhai', 'NN'), (',', ','), ('dept', 'NN'), ('.', '.')]\n",
      "[('space', 'NN'), (',', ','), ('professor', 'NN'), ('satish', 'JJ'), ('dhawan', 'NN'), (',', ','), ('succeed', 'VB'), ('dr.', 'JJ'), ('brahm', 'NN'), ('prakash', 'NN'), (',', ','), ('father', 'RB'), ('nuclear', 'JJ'), ('material', 'NN'), ('.', '.')]\n",
      "[('lucky', 'JJ'), ('work', 'NN'), ('three', 'CD'), ('closely', 'RB'), ('consider', 'VBP'), ('great', 'JJ'), ('opportunity', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('hyderabad', 'NN'), ('give', 'JJ'), ('lecture', 'NN'), (',', ','), ('14', 'CD'), ('year-old', 'JJ'), ('girl', 'NN'), ('ask', 'NN'), ('autograph', 'NN'), ('.', '.')]\n",
      "[('ask', 'NN'), ('goal', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('reply', 'NN'), (':', ':'), ('want', 'VBP'), ('live', 'JJ'), ('develop', 'NN'), ('india', 'NN'), ('.', '.')]\n",
      "[(',', ','), ('build', 'VB'), ('develop', 'VB'), ('india', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('proclaim', 'VB'), ('india', 'JJ'), ('underdevelop', 'JJ'), ('nation', 'NN'), (';', ':'), ('highly', 'RB'), ('develop', 'VB'), ('nation', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "## Apply Stopwords and Filter and then see every word parts of speech\n",
    "for i in range(len(sentenses)):\n",
    "    words = nltk.word_tokenize(sentenses[i])\n",
    "    words = [word for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    # sentenses[i] = ' '.join(words) #Converting words into sentences again\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2fcda27b-5312-449e-917d-bb8c86742d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('place', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(\"Taj Mahal is a beautiful place\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365d100-41e3-48ad-8e47-6091ad22bc5b",
   "metadata": {},
   "source": [
    "# Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3b3a51ac-4e0c-449f-8561-f1c8d75b67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"On March 3, 2024, John Smith, a data scientist at Google, traveled from New York City to San Francisco for a conference on Artificial Intelligence. The event, organized by OpenAI, was held at the Moscone Center and featured speakers from companies like Microsoft and Amazon. During his visit, John stayed at the Hilton Hotel and dined at Joe’s Seafood on Market Street.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "71fbf753-51a8-443f-ac4f-1231bb17e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ac884d9e-889a-4a63-9b93-2bb62ccabd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "69ff3a5c-1fab-4b7a-a47f-daa456203c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf2965-c5cb-461c-9dfb-c2f16ad7721b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
